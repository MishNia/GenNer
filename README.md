# GenNer

The advent of natural language processing (NLP) has significantly transformed the way we extract and interpret information from textual data. Named Entity Recognition (NER) stands out as a crucial task in NLP, enabling the identification and categorization of predefined entities within text. Traditional NER models excel in domain-specific tasks but falter when applied across diverse domains due to their training on domain-restricted datasets. This research proposes the development of a generalized NER model, leveraging the strengths of BERT and GPT, to perform efficiently across varied textual domains such as biomedicine, law, finance, and news articles. Our approach encompasses mission-specific instruction tuning and comprehensive data processing to address the challenges of entity diversity and domain variability.


# Training Data

The Pile is a large, diverse, open source language modelling data set that consists of many smaller datasets combined together. The objective is to obtain text from as many modalities as possible to ensure that models trained using The Pile will have much broader generalization abilities.

[Pile corpus](https://github.com/EleutherAI/the-pile)


